{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8712c091-9610-40e1-81d3-f3768c2d89f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import function as fu\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix, roc_auc_score, roc_curve, f1_score, precision_score, recall_score, matthews_corrcoef\n",
    "import sklearn as sk\n",
    "import copy\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "import math\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from random import SystemRandom   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b388a258-071d-450b-a559-4f86b7fb4eb0",
   "metadata": {},
   "source": [
    "## Synthetic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c045e20d-5801-494b-9621-c39356f70ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_list = [1,2,3,4,5]\n",
    "\n",
    "c_01 = 5\n",
    "\n",
    "comb =[(0.1,0.2), (0.1, 0.8), (0.2, 0.5) , (0.6 , 1.1),  (0.7 ,1.0) ,(0.8 ,1.4) ,( 0.9 ,1.1), (1.1, 1.3) ,(1.5 ,1.8)]\n",
    "\n",
    "for N1_ratio,N2_ratio in comb:\n",
    "    for initial_p in p_list:\n",
    "        fu.algorithm(c_01,initial_p, epochs=10, total_run=30,N1_ratio=N1_ratio,N2_ratio=N2_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b05e3a8-4280-4e05-941a-69effd52714f",
   "metadata": {},
   "source": [
    "## KTCC Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245201ca-fe3a-4082-ba8c-ab846f6b91bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KTCC\n",
    "# data preprocessing\n",
    "\n",
    "data = pd.read_csv('data/TelcoChurn.csv')\n",
    "\n",
    "# data type change\n",
    "l1 = [len(i.split()) for i in data['TotalCharges']]\n",
    "l2 = [i for i in range(len(l1)) if l1[i] != 1]\n",
    "\n",
    "for i in l2:\n",
    "    data.loc[i,'TotalCharges'] = data.loc[(i-1),'TotalCharges']\n",
    "    \n",
    "data['TotalCharges'] = data['TotalCharges'].astype(float)\n",
    "data.drop(columns = ['customerID'], inplace = True)\n",
    "\n",
    "# Label Encoder Transformation\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "df1 = data.copy(deep = True)\n",
    "text_data_features = [i for i in list(data.columns) if i not in list(data.describe().columns)]\n",
    "\n",
    "for i in text_data_features :\n",
    "    df1[i] = le.fit_transform(df1[i])\n",
    "\n",
    "col = list(df1.columns)\n",
    "categorical_features = []\n",
    "numerical_features = []\n",
    "for i in col:\n",
    "    if len(data[i].unique()) > 6:\n",
    "        numerical_features.append(i)\n",
    "    else:\n",
    "        categorical_features.append(i)\n",
    "\n",
    "categorical_features.remove('Churn')\n",
    "\n",
    "l1 = ['gender','SeniorCitizen','Partner','Dependents'] # Customer Information\n",
    "l2 = ['PhoneService','MultipleLines','InternetService','StreamingTV','StreamingMovies',\n",
    "      'OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport'] # Services Signed Up for!\n",
    "l3 = ['Contract','PaperlessBilling','PaymentMethod'] # Payment Information\n",
    "\n",
    "df1['MonthlyCharges_Group'] = [int(i / 5) for i in df1['MonthlyCharges']]\n",
    "df1['TotalCharges_Group'] = [int(i / 500) for i in df1['TotalCharges']]\n",
    "\n",
    "mms = MinMaxScaler() # Normalization\n",
    "ss = StandardScaler() # Standardization\n",
    "\n",
    "df1.drop(columns = ['MonthlyCharges_Group','TotalCharges_Group'], inplace = True)\n",
    "\n",
    "df1['tenure'] = mms.fit_transform(df1[['tenure']])\n",
    "df1['MonthlyCharges'] = mms.fit_transform(df1[['MonthlyCharges']])\n",
    "df1['TotalCharges'] = mms.fit_transform(df1[['TotalCharges']])\n",
    "\n",
    "\n",
    "df1.drop(columns = ['PhoneService', 'gender','StreamingTV','StreamingMovies','MultipleLines','InternetService'],inplace = True)\n",
    "\n",
    "f1 = df1.iloc[:,:13].values\n",
    "t1 = df1.iloc[:,13].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(f1, t1, test_size=0.2, random_state=1)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def Model(p,X_train, y_train, X_test, X_valid, y_valid):\n",
    "    reset_random_seeds()\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(13, activation='relu', input_shape = (np.array(X_train).shape[-1],)),\n",
    "    tf.keras.layers.Dense(6, activation='relu'),\n",
    "    tf.keras.layers.Dense(3, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid'),\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "      optimizer=tf.keras.optimizers.Adam(),\n",
    "      loss=keras.losses.BinaryCrossentropy()\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train,y_train, epochs=30, validation_data = (X_valid,y_valid) ,shuffle = True, batch_size=8 , class_weight =  {0: 1, 1: p}, verbose =0)\n",
    "\n",
    "    y_train_pred = model.predict(X_train, batch_size=8, verbose =0)\n",
    "    y_test_pred = model.predict(X_test, batch_size=8, verbose =0)\n",
    "    \n",
    "    return y_train_pred, y_test_pred\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "def reset_random_seeds():\n",
    "    os.environ['PYTHONHASHSEED']=str(1)\n",
    "    tf.random.set_seed(1)\n",
    "    np.random.seed(1)\n",
    "    random.seed(1)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def algorithm(c_01,initial_p, epochs, total_run,N1_ratio,N2_ratio):\n",
    "\n",
    "    # initial setting\n",
    "\n",
    "    D_n_train = np.unique(y_train, return_counts = True)[1][0]\n",
    "    D_p_train = np.unique(y_train, return_counts = True)[1][1]\n",
    "    \n",
    "    D_n_test = int(len(y_test) * D_n_train/len(y_train))\n",
    "    D_p_test = int(len(y_test) * D_p_train/len(y_train))\n",
    "\n",
    "\n",
    "    N1 = int(N1_ratio * len(y_test) / len(y_train) * D_p_train)\n",
    "    N2 = int(N2_ratio * len(y_test) / len(y_train) * D_p_train)\n",
    "   \n",
    "    \n",
    "    \n",
    "    list_minp = []\n",
    "    list_mincost = []\n",
    "    list_num1 = []\n",
    "    list_N1N2 = []\n",
    "    \n",
    "    list_conmat = []\n",
    "    \n",
    "    list_precision = []\n",
    "    list_recall = []\n",
    "    list_f1 = []\n",
    "    \n",
    "    list_auc = []\n",
    "    list_mcc = []\n",
    "    list_gmean = []\n",
    " \n",
    "    y_train_pred, y_test_pred = Model(initial_p,X_train, y_train, X_test, X_valid, y_valid)\n",
    "    fpr, fnr, thresholds = fpr_fnr(y_train_pred, y_train, y_test)\n",
    "       \n",
    "    y_test_classification, number_of_1 = threshold_change(c_01, initial_p, X_train,y_train, X_test, y_test, X_valid, y_valid, y_train_pred, y_test_pred, N1,N2)\n",
    "    TN, FN, FP, TP = confusion_matrix(y_test, y_test_classification).T.flatten()\n",
    "    initial_cost = int(c_01 *FN + FP) \n",
    "    \n",
    "    rho = 0.07\n",
    "    b = 1.6\n",
    "    \n",
    "    t= 100\n",
    "    alpha = 0.9\n",
    "    p_max = 5\n",
    "    p_min = 0\n",
    "\n",
    "    N =total_run\n",
    "\n",
    "    for e in range(epochs):\n",
    "\n",
    "        list_cost = []\n",
    "        list_p = []\n",
    "        p_new = initial_p\n",
    "        p = initial_p\n",
    "        \n",
    "        C_right = initial_cost * 3\n",
    "        C_left = initial_cost * 3\n",
    "        C_feasible = initial_cost * 3\n",
    "\n",
    "        \n",
    "        delta = 0\n",
    "        C_last = 0\n",
    "        delta_rho = 0\n",
    "        \n",
    "        for i in range(total_run):\n",
    "            \n",
    "            y_train_pred, y_test_pred = Model(p,X_train, y_train, X_test, X_valid, y_valid)\n",
    "            fpr, fnr, thresholds = fpr_fnr(y_train_pred, y_train, y_test)\n",
    "\n",
    "            interfpr_con, interfnr_con, threshold_con = find_cost_intersection_within(c_01,y_train, y_test, fpr, fnr, thresholds,N1,N2)\n",
    "            \n",
    "            (interfpr_opt, interfnr_opt, threshold_opt) = find_cost_intersection(c_01,y_train, y_test, fpr, fnr, thresholds)\n",
    "            (interfpr1,interfnr1,threshold1), (interfpr2,interfnr2,threshold2)= find_limit_point(fpr, fnr, thresholds,N1,N2)\n",
    "                \n",
    "            if interfpr2 < interfpr_opt:\n",
    "                y_test_classification, number_of_1 = fu.threshold_change(c_01, p, X_train,y_train, X_test, y_test, X_valid, y_valid, y_train_pred, y_test_pred, N1,N2)\n",
    "                TN, FN, FP, TP = confusion_matrix(y_test, y_test_classification).T.flatten()\n",
    "                C= int(c_01 *FN + FP) \n",
    "                C_right = min(C, C_right)\n",
    "\n",
    "            elif interfpr_opt < interfpr1:\n",
    "                y_test_classification, number_of_1 = fu.threshold_change(c_01, p, X_train,y_train, X_test, y_test, X_valid, y_valid, y_train_pred, y_test_pred, N1,N2)\n",
    "                TN, FN, FP, TP = confusion_matrix(y_test, y_test_classification).T.flatten()\n",
    "                C= int(c_01 *FN + FP) \n",
    "                C_left = min(C, C_left)\n",
    "\n",
    "            else:\n",
    "                y_test_classification, number_of_1 = fu.threshold_change(c_01, p, X_train,y_train, X_test, y_test, X_valid, y_valid, y_train_pred, y_test_pred, N1,N2)\n",
    "                TN, FN, FP, TP = confusion_matrix(y_test, y_test_classification).T.flatten()\n",
    "                C= int(c_01 *FN + FP) \n",
    "                C_feasible = min(C_feasible, C)\n",
    "          \n",
    "            # 3\n",
    "            t = alpha * t\n",
    "            \n",
    "            # 4\n",
    "            C_opt = min(C_left, C_right, C_feasible)\n",
    "            \n",
    "            # 5\n",
    "            delta_C = C - C_last \n",
    "            \n",
    "            # 6\n",
    "            probability = (C_right + 0.5 * C_feasible) / (C_right + C_left + C_feasible) \n",
    "            \n",
    "            # 9\n",
    "            probability2 = np.exp(-delta_C / t)\n",
    "            if delta_C <= 0 or ( delta_C > 0 and  probability2 > SystemRandom().random() ):\n",
    "            \n",
    "                # 7\n",
    "            \n",
    "                delta_rho = (1 - rho  ** ( (1 - (i/N) ) ** b ) )\n",
    "                \n",
    "                # if opt_point in right side, class weight p should be increased to predict less 1.\n",
    "                if probability <= SystemRandom().random():\n",
    "                    delta = (p_max - p) * delta_rho   # 8\n",
    "                    p_new = p + delta\n",
    "                else:\n",
    "                    delta = (p - p_min) *  delta_rho  # 8\n",
    "                    p_new = p - delta\n",
    "\n",
    "            list_p.append(p)\n",
    "            list_cost.append(C)\n",
    "            \n",
    "            p = p_new\n",
    "            C_last = C\n",
    "        \n",
    "        min_cost = min(list_cost)\n",
    "        min_p = list_p[list_cost.index(min_cost)]\n",
    "\n",
    "        y_train_pred, y_test_pred = Model(min_p,X_train, y_train, X_test, X_valid, y_valid)\n",
    "        fpr, fnr, thresholds = fpr_fnr(y_train_pred, y_train, y_test)\n",
    "\n",
    "        interfpr_con, interfnr_con, threshold_con = find_cost_intersection_within(c_01,y_train, y_test, fpr, fnr, thresholds,N1,N2)\n",
    "\n",
    "        y_test_classification, number_of_1 = threshold_change(c_01, p, X_train,y_train, X_test, y_test, X_valid, y_valid, y_train_pred, y_test_pred, N1,N2)\n",
    "        \n",
    "        precision,recall,f1, auc,mcc, gmean, con_mat = metrics(y_test, y_test_classification)\n",
    "        \n",
    "        min_p = np.round(min_p,5)\n",
    "        list_minp.append(min_p)\n",
    "        list_mincost.append(min_cost)\n",
    "        list_num1.append(number_of_1)\n",
    "        \n",
    "        list_precision.append(precision)\n",
    "        list_recall.append(recall)\n",
    "        list_f1.append(f1)\n",
    "        list_auc.append(auc)\n",
    "        list_mcc.append(mcc)\n",
    "        list_gmean.append(gmean)\n",
    "        \n",
    "        list_conmat.append(con_mat)\n",
    "        list_N1N2.append([N1, N2])\n",
    "        \n",
    "        dict = {'p': list_minp, 'cost': list_mincost, 'num1': list_num1, 'list_N1N2': list_N1N2, 'list_conmat': list_conmat, \n",
    "                'list_precision':list_precision, 'list_recall':list_recall, 'list_f1':list_f1, 'list_auc': list_auc, 'list_mcc': list_mcc, 'list_gmean': list_gmean}\n",
    "            \n",
    "        df = pd.DataFrame(dict) \n",
    "        method = 'on'\n",
    "\n",
    "    # saving the dataframe \n",
    "    df.to_csv('result/test_case1/on/{}_{}_{}_{}.csv'.format(method,N1_ratio,N2_ratio,initial_p))\n",
    "        \n",
    "    \n",
    "# ---------------------------------------------------------------------------\n",
    "def threshold_change(c_01, p, X_train,y_train, X_test, y_test, X_valid, y_valid, y_train_pred, y_test_pred, N1,N2):  \n",
    "\n",
    "    D_n_train = np.unique(y_train, return_counts = True)[1][0]\n",
    "    D_p_train = np.unique(y_train, return_counts = True)[1][1]\n",
    "    \n",
    "    D_n_test = int(len(y_test) * D_n_train/len(y_train))\n",
    "    D_p_test = int(len(y_test) * D_p_train/len(y_train))\n",
    "\n",
    "    fpr, fnr, thresholds = fpr_fnr(y_train_pred, y_train, y_test)\n",
    "     \n",
    "    interfpr_con, interfnr_con, threshold_con = find_cost_intersection_within(c_01,y_train, y_test, fpr, fnr, thresholds,N1,N2)\n",
    "\n",
    "    number_of_1 = D_n_test * interfpr_con + D_p_test * (1-interfnr_con)\n",
    "    number_of_1 = int(round(number_of_1))\n",
    "    \n",
    "    y_test_pred_copy = copy.copy(y_test_pred)\n",
    "    y_test_classification = np.where(y_test_pred_copy >= threshold_con,1,0)\n",
    "\n",
    "    if number_of_1 < N1:\n",
    "        y_test_pred_copy = copy.copy(y_test_pred)\n",
    "        num = 0\n",
    "\n",
    "        threshold_next = thresholds[(thresholds.index(threshold_con)) +1]\n",
    "        current_threshold_sum = sum(np.where(y_test_pred >= threshold_con,1,0))\n",
    "\n",
    "        for i in range(len(y_test_pred)):\n",
    "            if y_test_pred[i] ==  threshold_next:\n",
    "                y_test_pred_copy[i] = 1\n",
    "                num = num + 1\n",
    "\n",
    "                if current_threshold_sum + num == N1: break\n",
    "\n",
    "        y_test_classification = np.where(y_test_pred_copy > threshold_con,1,0)\n",
    "        TN, FN, FP, TP = confusion_matrix(y_test, y_test_classification).T.flatten()\n",
    "        number_of_1 = FP + TP\n",
    "        \n",
    "    \n",
    "    if N2 < number_of_1:\n",
    "        y_test_pred_copy = copy.copy(y_test_pred)\n",
    "        num = 0\n",
    "        \n",
    "        current_threshold_sum = sum(np.where(y_test_pred >= threshold_con,1,0))\n",
    "\n",
    "        for i in range(len(y_test_pred)):\n",
    "            if y_test_pred[i] == threshold_con:\n",
    "                y_test_pred_copy[i] = 0\n",
    "                num = num + 1\n",
    "\n",
    "                if current_threshold_sum - num == N2: \n",
    "                    break\n",
    "                \n",
    "        y_test_classification = np.where(y_test_pred_copy >= threshold_con,1,0)\n",
    "        TN, FN, FP, TP = confusion_matrix(y_test, y_test_classification).T.flatten()\n",
    "        number_of_1 = FP + TP\n",
    "\n",
    "    return y_test_classification, number_of_1\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "def metrics(y_test, y_test_classification):\n",
    "\n",
    "    TN, FN, FP, TP = confusion_matrix(y_test, y_test_classification).T.flatten()\n",
    "        \n",
    "    p = round(precision_score(y_test, y_test_classification),3)\n",
    "    r = round(recall_score(y_test, y_test_classification),3)\n",
    "    f = round(f1_score(y_test, y_test_classification),3)\n",
    "    auc = 1- round(roc_auc_score(y_test, y_test_classification),3)\n",
    "    mcc = round(matthews_corrcoef(y_test, y_test_classification),3)\n",
    "    \n",
    "    con_mat = confusion_matrix(y_test, y_test_classification).T.flatten().tolist()\n",
    "    tpr = TP / (TP+FN)\n",
    "    tnr = TN / (TN+FP)\n",
    "    gmean = np.sqrt(tpr * tnr)\n",
    "    \n",
    "    return p,r,f, auc,mcc, gmean, con_mat\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def find_limit_point(fpr, fnr, thresholds,N1,N2):\n",
    "    (interfpr1,interfnr1,threshold1) = (fpr[N1-1], fnr[N1-1], thresholds[N1-1])\n",
    "    (interfpr2,interfnr2,threshold2) = (fpr[N2-1], fnr[N2-1], thresholds[N2-1])\n",
    "    \n",
    "    return (interfpr1,interfnr1,threshold1), (interfpr2,interfnr2,threshold2)\n",
    "    \n",
    "# ---------------------------------------------------------------------------------------------------------------\n",
    "def fpr_fnr(y_train_pred, y_train, y_test):\n",
    "    \n",
    "    list_fpr = []\n",
    "    list_fnr = []\n",
    "    list_th = []\n",
    "    y_train_pred_sort =sorted(y_train_pred, reverse=True)\n",
    "    y_train_pred_sort[0] = np.array([1])\n",
    "    y_train_pred_sort[-1] = np.array([0])\n",
    "\n",
    "    for i in range(1,len(y_train_pred)+1):\n",
    "        if i % round(len(y_train) /len(y_test)) ==0:\n",
    "        \n",
    "            th = y_train_pred_sort[i-1][0]\n",
    "            y_result = np.where(y_train_pred >= th,1,0)\n",
    "            tn, fn, fp, tp = confusion_matrix(y_train, y_result).T.ravel()\n",
    "            fpr = fp / (tn+fp)\n",
    "            fnr = fn / (tp+fn)\n",
    "            \n",
    "            list_fpr.append(fpr)\n",
    "            list_fnr.append(fnr)\n",
    "            list_th.append(th)\n",
    "            \n",
    "    return [list_fpr, list_fnr, list_th]\n",
    "\n",
    "    \n",
    "    \n",
    "# find optimal point in whole area\n",
    "def find_cost_intersection(c_01,y_train, y_test, fpr, fnr, thresholds):\n",
    "    list_cost = []\n",
    "    \n",
    "    D_n_train = np.unique(y_train, return_counts = True)[1][0]\n",
    "    D_p_train = np.unique(y_train, return_counts = True)[1][1]\n",
    "\n",
    "    D_n_test = int(len(y_test) * D_n_train/len(y_train))\n",
    "    D_p_test = int(len(y_test) * D_p_train/len(y_train))\n",
    "\n",
    "    for i in range(len(fpr)): list_cost.append(c_01 * fpr[i] * D_n_test + fnr[i] * D_p_test)\n",
    "        \n",
    "    index = list_cost.index(min(list_cost))\n",
    "\n",
    "    return fpr[index], fnr[index], thresholds[index]\n",
    "# ---------------------------------------------------------------------------------------------------------------\n",
    "# find constrained optimal point within feasible area\n",
    "def find_cost_intersection_within(c_01,y_train, y_test, fpr, fnr, thresholds,N1,N2):\n",
    "\n",
    "    list_cost = []\n",
    "    \n",
    "    D_n_train = np.unique(y_train, return_counts = True)[1][0]\n",
    "    D_p_train = np.unique(y_train, return_counts = True)[1][1]\n",
    "\n",
    "    D_n_test = int(len(y_test) * D_n_train/len(y_train))\n",
    "    D_p_test = int(len(y_test) * D_p_train/len(y_train))\n",
    "    \n",
    "    (interfpr1,interfnr1,threshold1) = (fpr[N1-1], fnr[N1-1], thresholds[N1-1])\n",
    "    (interfpr2,interfnr2,threshold2) = (fpr[N2-1], fnr[N2-1], thresholds[N2-1])\n",
    "\n",
    "    for i in range(N1-1, N2): list_cost.append(c_01 * fpr[i] * D_n_test + fnr[i] * D_p_test)\n",
    "\n",
    "    index = list_cost.index(min(list_cost))\n",
    "    \n",
    "    return fpr[N1-1 + index], fnr[N1-1 + index], thresholds[N1-1 + index]\n",
    "# ---------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551ebe5b-cafb-49fa-b1d1-1a1dc9681d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_list = [1,2,3,4,5]\n",
    "\n",
    "c_01 = 5\n",
    "\n",
    "D_n_train = np.unique(y_train, return_counts = True)[1][0]\n",
    "D_p_train = np.unique(y_train, return_counts = True)[1][1]\n",
    "\n",
    "D_n_test = int(len(y_test) * D_n_train/len(y_train))\n",
    "D_p_test = int(len(y_test) * D_p_train/len(y_train))\n",
    "\n",
    "N1 = int(N1_ratio * len(y_test) / len(y_train) * D_p_train)\n",
    "N2 = int(N2_ratio * len(y_test) / len(y_train) * D_p_train)\n",
    "\n",
    "for N1_ratio,N2_ratio in comb:\n",
    "    for initial_p in p_list:\n",
    "        fu.algorithm(c_01,initial_p, epochs=10, total_run=30,N1_ratio=N1_ratio,N2_ratio=N2_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54a58a5-8c7b-4275-8163-b087198a68a4",
   "metadata": {},
   "source": [
    "## DCCC Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c81667-9a15-4247-b72d-6427ac73fd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DCCC dataset\n",
    "\n",
    "df = pd.read_csv('data/creditcard.csv')\n",
    "\n",
    "df['IsDefaulter'] =df ['default.payment.next.month']\n",
    "df = df.drop('default.payment.next.month',axis = 1)\n",
    "\n",
    "fil = (df['EDUCATION'] == 5) | (df['EDUCATION'] == 6) | (df['EDUCATION'] == 0)\n",
    "df.loc[fil, 'EDUCATION'] = 4\n",
    "\n",
    "fil = df['MARRIAGE'] == 0\n",
    "df.loc[fil, 'MARRIAGE'] = 3\n",
    "\n",
    "categorical_features = ['SEX', 'EDUCATION', 'MARRIAGE']\n",
    "df_cat = df[categorical_features]\n",
    "df_cat['Defaulter'] = df['IsDefaulter']\n",
    "\n",
    "df_cat.replace({'SEX': {1 : 'MALE', 2 : 'FEMALE'}, 'EDUCATION' : {1 : 'graduate school', 2 : 'university', 3 : 'high school', 4 : 'others'}, 'MARRIAGE' : {1 : 'married', 2 : 'single', 3 : 'others'}}, inplace = True)\n",
    "\n",
    "df.rename(columns={'PAY_0':'PAY_SEPT','PAY_2':'PAY_AUG','PAY_3':'PAY_JUL','PAY_4':'PAY_JUN','PAY_5':'PAY_MAY','PAY_6':'PAY_APR'},inplace=True)\n",
    "df.rename(columns={'BILL_AMT1':'BILL_AMT_SEPT','BILL_AMT2':'BILL_AMT_AUG','BILL_AMT3':'BILL_AMT_JUL','BILL_AMT4':'BILL_AMT_JUN','BILL_AMT5':'BILL_AMT_MAY','BILL_AMT6':'BILL_AMT_APR'}, inplace = True)\n",
    "df.rename(columns={'PAY_AMT1':'PAY_AMT_SEPT','PAY_AMT2':'PAY_AMT_AUG','PAY_AMT3':'PAY_AMT_JUL','PAY_AMT4':'PAY_AMT_JUN','PAY_AMT5':'PAY_AMT_MAY','PAY_AMT6':'PAY_AMT_APR'},inplace=True)\n",
    "\n",
    "df['AGE']=df['AGE'].astype('int')\n",
    "df.groupby('IsDefaulter')['AGE'].mean()\n",
    "df = df.astype('int')\n",
    "\n",
    "bill_amnt_df = df[['BILL_AMT_SEPT',\t'BILL_AMT_AUG',\t'BILL_AMT_JUL',\t'BILL_AMT_JUN',\t'BILL_AMT_MAY',\t'BILL_AMT_APR']]\n",
    "pay_amnt_df = df[['PAY_AMT_SEPT',\t'PAY_AMT_AUG',\t'PAY_AMT_JUL',\t'PAY_AMT_JUN',\t'PAY_AMT_MAY',\t'PAY_AMT_APR', 'IsDefaulter']]\n",
    "\n",
    "columns = list(df.columns)\n",
    "columns.pop()\n",
    "\n",
    "X, y = df.iloc[:,0:-1], df['IsDefaulter']\n",
    "\n",
    "df_fr = X.copy()\n",
    "df_fr['IsDefaulter'] = y\n",
    "df_fr['Payement_Value'] = df_fr['PAY_SEPT'] + df_fr['PAY_AUG'] + df_fr['PAY_JUL'] + df_fr['PAY_JUN'] + df_fr['PAY_MAY'] + df_fr['PAY_APR']\n",
    "\n",
    "df_fr['Dues'] = (df_fr['BILL_AMT_APR']+df_fr['BILL_AMT_MAY']+df_fr['BILL_AMT_JUN']+df_fr['BILL_AMT_JUL']+df_fr['BILL_AMT_SEPT'])-(df_fr['PAY_AMT_APR']+df_fr['PAY_AMT_MAY']+df_fr['PAY_AMT_JUN']+df_fr['PAY_AMT_JUL']+df_fr['PAY_AMT_AUG']+df_fr['PAY_AMT_SEPT'])\n",
    "\n",
    "df_fr['EDUCATION']=np.where(df_fr['EDUCATION'] == 6, 4, df_fr['EDUCATION'])\n",
    "df_fr['EDUCATION']=np.where(df_fr['EDUCATION'] == 0, 4, df_fr['EDUCATION'])\n",
    "\n",
    "df_fr['MARRIAGE']=np.where(df_fr['MARRIAGE'] == 0, 3, df_fr['MARRIAGE'])\n",
    "\n",
    "df_fr.replace({'SEX': {1 : 'MALE', 2 : 'FEMALE'}, 'EDUCATION' : {1 : 'graduate school', 2 : 'university', 3 : 'high school', 4 : 'others'}, 'MARRIAGE' : {1 : 'married', 2 : 'single', 3 : 'others'}}, inplace = True)\n",
    "\n",
    "df_fr = pd.get_dummies(df_fr,columns=['EDUCATION','MARRIAGE'])\n",
    "\n",
    "df_fr.drop(['EDUCATION_others','MARRIAGE_others'],axis = 1, inplace = True)\n",
    "\n",
    "df_fr = pd.get_dummies(df_fr, columns = ['PAY_SEPT',\t'PAY_AUG',\t'PAY_JUL',\t'PAY_JUN',\t'PAY_MAY',\t'PAY_APR'], drop_first = True )\n",
    "\n",
    "encoders_nums = {\n",
    "                 \"SEX\":{\"FEMALE\": 0, \"MALE\": 1}\n",
    "}\n",
    "df_fr = df_fr.replace(encoders_nums)\n",
    "\n",
    "df_fr.drop('ID',axis = 1, inplace = True)\n",
    "#df_fr = df_fr[:7500]\n",
    "\n",
    "X = df_fr.drop(['IsDefaulter','Payement_Value','Dues'],axis=1)\n",
    "y = df_fr['IsDefaulter']\n",
    "\n",
    "columns = X.columns\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "initial_p = 1\n",
    "c_10 = 1\n",
    "# -------------------------------------------------------------------------------------\n",
    "\n",
    "def reset_random_seeds():\n",
    "    os.environ['PYTHONHASHSEED']=str(1)\n",
    "    tf.random.set_seed(1)\n",
    "    np.random.seed(1)\n",
    "    random.seed(1)\n",
    "\n",
    "\n",
    "\n",
    "def Model(p,X_train, y_train, X_test, X_valid, y_valid):\n",
    "    reset_random_seeds()\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(78, activation='relu', input_shape = (np.array(X_train).shape[-1],)),\n",
    "    tf.keras.layers.Dense(34, activation='relu'),\n",
    "    tf.keras.layers.Dense(17, activation='relu'),\n",
    "    tf.keras.layers.Dense(8, activation='relu'),\n",
    "    tf.keras.layers.Dense(4, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid'),\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "      optimizer=tf.keras.optimizers.Adam(),\n",
    "      loss=keras.losses.BinaryCrossentropy()\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train,y_train, epochs=30, validation_data = (X_valid,y_valid) ,shuffle = True, batch_size=8 , class_weight =  {0: 1, 1: p}, verbose =0)\n",
    "\n",
    "    y_train_pred = model.predict(X_train, batch_size=8, verbose =0)\n",
    "    y_test_pred = model.predict(X_test, batch_size=8, verbose =0)\n",
    "    \n",
    "    return y_train_pred, y_test_pred\n",
    "\n",
    "\n",
    "\n",
    "def algorithm(c_01,initial_p, epochs, total_run,N1_ratio,N2_ratio):\n",
    "\n",
    "    # initial setting\n",
    "\n",
    "    D_n_train = np.unique(y_train, return_counts = True)[1][0]\n",
    "    D_p_train = np.unique(y_train, return_counts = True)[1][1]\n",
    "    \n",
    "    D_n_test = int(len(y_test) * D_n_train/len(y_train))\n",
    "    D_p_test = int(len(y_test) * D_p_train/len(y_train))\n",
    "\n",
    "\n",
    "    N1 = int(N1_ratio * len(y_test) / len(y_train) * D_p_train)\n",
    "    N2 = int(N2_ratio * len(y_test) / len(y_train) * D_p_train)\n",
    "   \n",
    "    \n",
    "    \n",
    "    list_minp = []\n",
    "    list_mincost = []\n",
    "    list_num1 = []\n",
    "    list_N1N2 = []\n",
    "    \n",
    "    list_conmat = []\n",
    "    \n",
    "    list_precision = []\n",
    "    list_recall = []\n",
    "    list_f1 = []\n",
    "    \n",
    "    list_auc = []\n",
    "    list_mcc = []\n",
    "    list_gmean = []\n",
    " \n",
    "    y_train_pred, y_test_pred = Model(initial_p,X_train, y_train, X_test, X_valid, y_valid)\n",
    "    fpr, fnr, thresholds = fpr_fnr(y_train_pred, y_train, y_test)\n",
    "       \n",
    "    interfpr_con, interfnr_con, threshold_con = find_cost_intersection_within(c_01,y_train, y_test, fpr, fnr, thresholds,N1,N2)\n",
    "        \n",
    "    y_test_classification, number_of_1 = threshold_change(c_01, initial_p, X_train,y_train, X_test, y_test, X_valid, y_valid, y_train_pred, y_test_pred, N1,N2)\n",
    "    TN, FN, FP, TP = confusion_matrix(y_test, y_test_classification).T.flatten()\n",
    "    initial_cost = int(c_01 *FN + FP) \n",
    "    \n",
    "    rho = 0.07\n",
    "    b = 1.6\n",
    "    \n",
    "    t= 100\n",
    "    alpha = 0.9\n",
    "    p_max = 5\n",
    "    p_min = 0\n",
    "\n",
    "    N =total_run\n",
    "\n",
    "    for e in range(epochs):\n",
    "\n",
    "        list_cost = []\n",
    "        list_p = []\n",
    "        p_new = initial_p\n",
    "        p = initial_p\n",
    "        \n",
    "        C_right = initial_cost * 3\n",
    "        C_left = initial_cost * 3\n",
    "        C_feasible = initial_cost * 3\n",
    "\n",
    "        \n",
    "        delta = 0\n",
    "        C_last = 0\n",
    "        delta_rho = 0\n",
    "        \n",
    "        for i in range(total_run):\n",
    "            \n",
    "            y_train_pred, y_test_pred = Model(p,X_train, y_train, X_test, X_valid, y_valid)\n",
    "            fpr, fnr, thresholds = fpr_fnr(y_train_pred, y_train, y_test)\n",
    "\n",
    "            interfpr_con, interfnr_con, threshold_con = find_cost_intersection_within(c_01,y_train, y_test, fpr, fnr, thresholds,N1,N2)\n",
    "            \n",
    "            (interfpr_opt, interfnr_opt, threshold_opt) = find_cost_intersection(c_01,y_train, y_test, fpr, fnr, thresholds)\n",
    "            (interfpr1,interfnr1,threshold1), (interfpr2,interfnr2,threshold2)= find_limit_point(fpr, fnr, thresholds,N1,N2)\n",
    "                \n",
    "            if interfpr2 < interfpr_opt:\n",
    "                y_test_classification, number_of_1 = fu.threshold_change(c_01, p, X_train,y_train, X_test, y_test, X_valid, y_valid, y_train_pred, y_test_pred, N1,N2)\n",
    "                TN, FN, FP, TP = confusion_matrix(y_test, y_test_classification).T.flatten()\n",
    "                C= int(c_01 *FN + FP) \n",
    "                C_right = min(C, C_right)\n",
    "\n",
    "            elif interfpr_opt < interfpr1:\n",
    "                y_test_classification, number_of_1 = fu.threshold_change(c_01, p, X_train,y_train, X_test, y_test, X_valid, y_valid, y_train_pred, y_test_pred, N1,N2)\n",
    "                TN, FN, FP, TP = confusion_matrix(y_test, y_test_classification).T.flatten()\n",
    "                C= int(c_01 *FN + FP) \n",
    "                C_left = min(C, C_left)\n",
    "\n",
    "            else:\n",
    "                y_test_classification, number_of_1 = fu.threshold_change(c_01, p, X_train,y_train, X_test, y_test, X_valid, y_valid, y_train_pred, y_test_pred, N1,N2)\n",
    "                TN, FN, FP, TP = confusion_matrix(y_test, y_test_classification).T.flatten()\n",
    "                C= int(c_01 *FN + FP) \n",
    "                C_feasible = min(C_feasible, C)\n",
    "          \n",
    "            # 3\n",
    "            t = alpha * t\n",
    "            \n",
    "            # 4\n",
    "            C_opt = min(C_left, C_right, C_feasible)\n",
    "            \n",
    "            # 5\n",
    "            delta_C = C - C_last \n",
    "            \n",
    "            # 6\n",
    "            probability = (C_right + 0.5 * C_feasible) / (C_right + C_left + C_feasible) \n",
    "            \n",
    "            # 9\n",
    "            probability2 = np.exp(-delta_C / t)\n",
    "            if delta_C <= 0 or ( delta_C > 0 and  probability2 > SystemRandom().random() ):\n",
    "            \n",
    "                # 7\n",
    "            \n",
    "                delta_rho = (1 - rho  ** ( (1 - (i/N) ) ** b ) )\n",
    "                \n",
    "                # if opt_point in right side, class weight p should be increased to predict less 1.\n",
    "                if probability <= SystemRandom().random():\n",
    "                    delta = (p_max - p) * delta_rho   # 8\n",
    "                    p_new = p + delta\n",
    "                else:\n",
    "                    delta = (p - p_min) *  delta_rho  # 8\n",
    "                    p_new = p - delta\n",
    "\n",
    "\n",
    "            list_p.append(p)\n",
    "            list_cost.append(C)\n",
    "            \n",
    "            p = p_new\n",
    "            C_last = C\n",
    "        \n",
    "        min_cost = min(list_cost)\n",
    "        min_p = list_p[list_cost.index(min_cost)]\n",
    "\n",
    "        y_train_pred, y_test_pred = Model(min_p,X_train, y_train, X_test, X_valid, y_valid)\n",
    "        fpr, fnr, thresholds = fpr_fnr(y_train_pred, y_train, y_test)\n",
    "\n",
    "        interfpr_con, interfnr_con, threshold_con = find_cost_intersection_within(c_01,y_train, y_test, fpr, fnr, thresholds,N1,N2)\n",
    "\n",
    "        y_test_classification, number_of_1 = threshold_change(c_01, p, X_train,y_train, X_test, y_test, X_valid, y_valid, y_train_pred, y_test_pred, N1,N2)\n",
    "        \n",
    "        precision,recall,f1, auc,mcc, gmean, con_mat = metrics(y_test, y_test_classification)\n",
    "        \n",
    "        min_p = np.round(min_p,5)\n",
    "        list_minp.append(min_p)\n",
    "        list_mincost.append(min_cost)\n",
    "        list_num1.append(number_of_1)\n",
    "        \n",
    "        list_precision.append(precision)\n",
    "        list_recall.append(recall)\n",
    "        list_f1.append(f1)\n",
    "        list_auc.append(auc)\n",
    "        list_mcc.append(mcc)\n",
    "        list_gmean.append(gmean)\n",
    "        \n",
    "        list_conmat.append(con_mat)\n",
    "        list_N1N2.append([N1, N2])\n",
    "        \n",
    "        dict = {'p': list_minp, 'cost': list_mincost, 'num1': list_num1, 'list_N1N2': list_N1N2, 'list_conmat': list_conmat, \n",
    "                'list_precision':list_precision, 'list_recall':list_recall, 'list_f1':list_f1, 'list_auc': list_auc, 'list_mcc': list_mcc, 'list_gmean': list_gmean}\n",
    "            \n",
    "        df = pd.DataFrame(dict) \n",
    "        method = 'on'\n",
    "\n",
    "    # saving the dataframe \n",
    "    df.to_csv('result/test_case2/on/{}_{}_{}_{}.csv'.format(method,N1_ratio,N2_ratio,initial_p))\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157df2b7-3b9b-4902-9ef3-9c1de4406cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_list = [1,2,3,4,5]\n",
    "\n",
    "\n",
    "c_01 = 5\n",
    "\n",
    "D_n_train = np.unique(y_train, return_counts = True)[1][0]\n",
    "D_p_train = np.unique(y_train, return_counts = True)[1][1]\n",
    "\n",
    "D_n_test = int(len(y_test) * D_n_train/len(y_train))\n",
    "D_p_test = int(len(y_test) * D_p_train/len(y_train))\n",
    "\n",
    "for N1_ratio,N2_ratio in comb:\n",
    "    for initial_p in p_list:\n",
    "        fu.algorithm(c_01,initial_p, epochs=10, total_run=30,N1_ratio=N1_ratio,N2_ratio=N2_ratio)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow-2.7.0",
   "language": "python",
   "name": "tensorflow-2.7.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
