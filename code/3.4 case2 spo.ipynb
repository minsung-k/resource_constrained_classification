{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import function as fu\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import recall_score, confusion_matrix, precision_score, f1_score, accuracy_score, classification_report\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix, roc_auc_score, roc_curve, f1_score, precision_score, recall_score, matthews_corrcoef\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/creditcard.csv')\n",
    "data.rename(columns= {'LIMIT_BAL':'given_credit',\n",
    "                       'PAY_0':'status_apr',\n",
    "                       'PAY_2':'status_may',\n",
    "                       'PAY_3':'status_jun',\n",
    "                       'PAY_4':'status_jul',\n",
    "                       'PAY_5':'status_aug',\n",
    "                       'PAY_6':'status_sep',\n",
    "\n",
    "                       'BILL_AMT1':'bill_apr',\n",
    "                       'BILL_AMT2':'bill_may',\n",
    "                       'BILL_AMT3':'bill_jun',\n",
    "                       'BILL_AMT4':'bill_jul',\n",
    "                       'BILL_AMT5':'bill_aug',\n",
    "                       'BILL_AMT6':'bill_sep',\n",
    "                       \n",
    "                       'PAY_AMT1':'pay_apr',\n",
    "                       'PAY_AMT2':'pay_may',\n",
    "                       'PAY_AMT3':'pay_jun',\n",
    "                       'PAY_AMT4':'pay_jul',\n",
    "                       'PAY_AMT5':'pay_aug',\n",
    "                       'PAY_AMT6':'pay_sep',\n",
    "                       },\n",
    "                        \n",
    "            inplace = True)\n",
    "\n",
    "# As a syntaxis rule, the rest of the column names are changed to lower case\n",
    "data.columns = data.columns.str.lower()\n",
    "\n",
    "#Blank spaces are also replaced by \"_\"\n",
    "data.columns = data.columns.str.replace(\" \", \"_\")\n",
    "\n",
    "#Our tatget variable has a very long name, so it is shortened to \"default_payment\"\n",
    "data.rename(columns= {'default.payment.next.month':'default_payment'}, inplace = True)\n",
    "\n",
    "marriage = pd.DataFrame()\n",
    "\n",
    "#Frequency table by marriage value\n",
    "marriage[\"qty_data\"] = pd.DataFrame(data.marriage.value_counts())\n",
    "\n",
    "#Mean age by marriage value:\n",
    "marriage[\"mean_age\"] = pd.DataFrame(round(data.groupby(\"marriage\")[\"age\"].mean(),2))\n",
    "\n",
    "#Mean default_payment\n",
    "marriage[\"deafult_rate\"] = pd.DataFrame(round(data.groupby(\"marriage\")[\"default_payment\"].mean(),2))\n",
    "\n",
    "data_clean = data\n",
    "data_clean.loc[(data_clean['marriage']==0),'marriage']=3\n",
    "\n",
    "education = pd.DataFrame()\n",
    "\n",
    "#Frequency table by marriage value\n",
    "education[\"qty_data\"] = pd.DataFrame(data.education.value_counts())\n",
    "\n",
    "#Mean age by marriage value:\n",
    "education[\"mean_age\"] = pd.DataFrame(round(data.groupby(\"education\")[\"age\"].mean(),2))\n",
    "\n",
    "#Mean default_payment\n",
    "education[\"deafult_rate\"] = pd.DataFrame(round(data.groupby(\"education\")[\"default_payment\"].mean(),2))\n",
    "\n",
    "data_clean.loc[(data_clean['education']==0) | (data_clean['education']==5) | (data_clean['education']==6),\n",
    "               'education']=4\n",
    "\n",
    "status = pd.DataFrame()\n",
    "\n",
    "#Frequency table by marriage value\n",
    "status[\"qty_data\"] = pd.DataFrame(data.status_apr.value_counts())\n",
    "\n",
    "#Mean age by marriage value:\n",
    "status[\"mean_age\"] = pd.DataFrame(round(data.groupby(\"status_apr\")[\"age\"].mean(),2))\n",
    "\n",
    "#Mean default_payment\n",
    "status[\"deafult_rate\"] = pd.DataFrame(round(data.groupby(\"status_apr\")[\"default_payment\"].mean(),2))\n",
    "\n",
    "data_FE = data_clean\n",
    "#Set \"education\" as catigory type\n",
    "data_FE[\"education\"] = data_FE [\"education\"].astype(\"category\")\n",
    "\n",
    "#Creating dummy features\n",
    "edu_dummies = pd.get_dummies(data_FE['education'], prefix='edu')\n",
    "\n",
    "#Adding these new dummy variables to our dataset\n",
    "data_FE = pd.concat( [data_FE, edu_dummies] , axis = 1)\n",
    "data_FE = data_FE.drop(columns=['education']);\n",
    "\n",
    "data_FE.columns = data_FE.columns.str.replace(\",\", \"-\") # subst. \",\" for \"-\"\n",
    "data_FE.columns = data_FE.columns.str.replace(\" \", \"\")  # deleting spaces\n",
    "data_FE.columns = data_FE.columns.str.replace(\"(\", \"\")  # deleting \"(\"\n",
    "data_FE.columns = data_FE.columns.str.replace(\")\", \"\")  # deleting \")\"\n",
    "data_FE.columns = data_FE.columns.str.replace(\"]\", \"\"); # deleting \"]\"\n",
    "\n",
    "data_premod = data_FE\n",
    "# Separating into input data and target variables\n",
    "\n",
    "#Eliminating the target column from the input data\n",
    "X_original =  pd.DataFrame( data_premod.drop(columns='default_payment'))\n",
    "\n",
    "# Target varibale as independent dataframe\n",
    "Y_original = pd.DataFrame( data_premod[\"default_payment\"] )\n",
    "\n",
    "# Standard Scaler from SciKit Learn applies the Z-score normalizaion to the input \"X_original\"\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_scaled = pd.DataFrame( scaler.fit_transform(X_original) )\n",
    "X_scaled.columns = X_original.columns;\n",
    "#X_scaled.iloc[:,1:13].describe()\n",
    "\n",
    "Y_scaled = Y_original;\n",
    "data_scaled= pd.concat([X_scaled, Y_scaled], axis=1, join='inner');\n",
    "    \n",
    "formatted_data_scaled = pd.DataFrame()\n",
    "formatted_data_scaled =  pd.melt(data_scaled, [\"default_payment\"], var_name=\"feature\");\n",
    "\n",
    "X_train, X_test, y_train, y_test= train_test_split(X_scaled, Y_scaled,test_size=0.2,shuffle = True,random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30min 48s, sys: 3min 10s, total: 33min 58s\n",
      "Wall time: 23min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "c_01 = 3\n",
    "c_10 = 1\n",
    "p_list = [2,4,6,8,10]\n",
    "N_list = [[0.2, 0.5], [0.9,1.1], [1.5, 1.8]]\n",
    "N_list1 = [ [0.1, 0.8], [0.2,0.5] , [0.6, 1.1]  , [0.7, 1] , [0.8, 1.2], [0.8, 1.4], [0.9,1.4] , [1, 1.1],  [1, 1.2] , [1.1,1.3], [1.5, 1.8] ]\n",
    "for N1_ratio,N2_ratio in N_list1:\n",
    "    N1= int(np.unique(y_test, return_counts = True)[1][1] * N1_ratio)\n",
    "    N2= int(np.unique(y_test, return_counts = True)[1][1] * N2_ratio)\n",
    "        \n",
    "    list_con_mat = []\n",
    "    list_p = []\n",
    "    list_cost = []\n",
    "    list_number = []\n",
    "    list_precision = []\n",
    "    list_recall = []\n",
    "    list_f1 = []\n",
    "    list_auc = []\n",
    "    list_mcc = []\n",
    "    list_gmean = []\n",
    "\n",
    "    for p in p_list:\n",
    "        ticket = True\n",
    "        while ticket:\n",
    "            con_mat,cost, number, precision, recall, f1, auc,mcc, gmean = algorithm_spo(c_01,p, N1,N2)\n",
    "            if not (con_mat[0] == N1) or (con_mat[-1] == N2): ticket = False\n",
    "\n",
    "        #print(f'initial_p:{p},cost:{cost},precision:{precision},recall:{recall},f1:{f1}')\n",
    "        list_p.append(p)\n",
    "        list_cost.append(cost)\n",
    "        list_number.append(number)\n",
    "        list_precision.append(precision)\n",
    "        list_recall.append(recall)\n",
    "        list_f1.append(f1)\n",
    "        list_con_mat.append(con_mat)\n",
    "        list_auc.append(auc)\n",
    "        list_mcc.append(mcc)\n",
    "        list_gmean.append(gmean)\n",
    "\n",
    "\n",
    "    dict = {'p': list_p, 'num1':list_number, 'con_mat':list_con_mat, 'cost': list_cost, 'list_precision':list_precision, 'list_recall':list_recall, 'list_f1':list_f1,\n",
    "           'list_auc': list_auc, 'list_mcc': list_mcc, 'list_gmean': list_gmean}   \n",
    "\n",
    "    df = pd.DataFrame(dict) \n",
    "    method = 'spo'\n",
    "    # saving the dataframe \n",
    "    df.to_csv('result/case2/result_{}_{}_{}.csv'.format(method,N1,N2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spo_plus_loss(c_01):\n",
    "    c_01 = tf.cast(c_01, tf.float64)\n",
    "    \n",
    "    def my_loss(y_true,y_pred):\n",
    "        #y_pred = tf.clip_by_value(y_pred, 0.0001, 1 - 0.0001)\n",
    "        y_pred = tf.cast(y_pred, tf.float64)\n",
    "        y_true = tf.cast(y_true, tf.float64)\n",
    "        \n",
    "        max_1 = tf.cast(tf.math.maximum(2 * y_pred - 1,0),tf.float64)\n",
    "        max_2 = tf.cast(tf.math.maximum(1-2 * y_pred, 0), tf.float64)\n",
    "        \n",
    "        loss =  y_pred*(1-y_pred)+ max_1 * c_01 * y_true + (1- y_true) * max_2 + c_01 * y_true * (1-2 * y_pred) + (1 - y_true) * (2*y_pred - 1)\n",
    "        #loss = tf.cast(y_true, tf.float64) * y_pred +(1-tf.cast(y_true, tf.float64)) * ( 1-y_pred)\n",
    "         \n",
    "        #if max_1 ==0: max_1 =tf.cast(1, tf.float64)\n",
    "        #if max_2 ==0: max_2 =tf.cast(1, tf.float64)\n",
    "            \n",
    "        #loss = tf.math.multiply_no_nan(x= tf.math.log(max_1), y= tf.cast(y_true, tf.float64))* c_01 +  tf.math.multiply_no_nan( x=tf.math.log(max_2), y= (1- tf.cast(y_true, tf.float64)) )*c_01\n",
    "\n",
    "        #loss = ((tf.cast(y_true, tf.float64) * (1-y_pred))*c_01 + ((1-tf.cast(y_true, tf.float64)) * y_pred ))\n",
    "                                                      \n",
    "        return loss\n",
    "    return my_loss\n",
    "\n",
    "def Model_spo(c_01,X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, activation='relu', input_shape = (np.array(X_train).shape[-1],)),\n",
    "    tf.keras.layers.Dense(5, activation='relu'),\n",
    "    tf.keras.layers.Dense(3, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid'),\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(), loss= spo_plus_loss(c_01))\n",
    "    \n",
    "    model.fit(X_train,y_train,shuffle = True, epochs= 30,verbose =0)\n",
    "    y_pred = model.predict(X_test, batch_size=16, verbose =0)\n",
    " \n",
    "    return y_pred\n",
    "\n",
    "def algorithm_spo(c_01,p, N1,N2):\n",
    "        \n",
    "    y_pred = Model_spo(c_01,X_train, y_train, X_test, y_test)\n",
    "    fpr, fnr, thresholds = fu.fpr_fnr(y_pred, y_test)\n",
    "\n",
    "    D_n = np.unique(y_test, return_counts = True)[1][0]\n",
    "    D_p = np.unique(y_test, return_counts = True)[1][1]\n",
    "    \n",
    "    if D_p <= N1:\n",
    "        num = N1\n",
    "    elif N2 <= D_p:\n",
    "        num = N2\n",
    "    elif N1 < D_p < N2:\n",
    "        num = D_p\n",
    "    \n",
    "    th = sorted(y_pred, reverse = True)[num]\n",
    "    y_result = np.where(y_pred>th, 1, 0)\n",
    "    TN, FN, FP, TP = confusion_matrix(y_test, y_result).T.flatten()\n",
    "\n",
    "    if (FP+TP) < N1:\n",
    "        ticket1 = False\n",
    "        ticket2 = False\n",
    "        count = 0\n",
    "        \n",
    "        target = N1 - (FP+TP) \n",
    "        for i in range(len(y_result)):\n",
    "            if y_result[i] ==0:\n",
    "                y_result[i] =1\n",
    "                count +=1\n",
    "            if count == target:\n",
    "                ticket1 = True\n",
    "\n",
    "            if ticket1 == True:\n",
    "                break\n",
    "\n",
    "    elif N2 < (FP+TP):\n",
    "        ticket1 = False\n",
    "        ticket2 = False\n",
    "        count = 0\n",
    "        \n",
    "        target = (FP+TP) - N2\n",
    "        for i in range(len(y_result)):\n",
    "            if y_result[i] ==1:\n",
    "                y_result[i] =0\n",
    "                count +=1\n",
    "            if count == target:\n",
    "                ticket2 = True\n",
    "\n",
    "            if ticket2 == True:\n",
    "                break\n",
    "\n",
    "    TN, FN, FP, TP = confusion_matrix(y_test, y_result).T.flatten()\n",
    "    cost = confusion_matrix(y_test, y_result).T[0][1] * c_01 + confusion_matrix(y_test, y_result).T[1][0] * c_10\n",
    "    cost = round(cost)\n",
    " \n",
    "    number = confusion_matrix(y_test, y_result).T[1][1] + confusion_matrix(y_test, y_result).T[1][0]       \n",
    "    p = round(precision_score(y_test, y_result),2)\n",
    "    r = round(recall_score(y_test, y_result),2)\n",
    "    f = round(f1_score(y_test, y_result),2)\n",
    "    auc = round(roc_auc_score(y_test, y_result),2)\n",
    "    mcc = round(matthews_corrcoef(y_test, y_result),2)\n",
    "    \n",
    "    tpr = TP / (TP+FN)\n",
    "    tnr = TN / (TN+FP)\n",
    "    gmean = np.sqrt(tpr * tnr)\n",
    "    \n",
    "    con_mat = confusion_matrix(y_test, y_result).T.flatten().tolist()\n",
    "    \n",
    "    return con_mat,cost, number, p, r, f, auc,mcc, gmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow-2.7.0",
   "language": "python",
   "name": "tensorflow-2.7.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
